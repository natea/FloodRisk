AI Model for City-Scale Pluvial Flood Depth Mapping: Proposed Approach

Introduction

Pluvial flooding — intense rainfall overwhelming local drainage — is a growing threat to cities, causing significant property damage and economic loss ￼. With climate change increasing heavy precipitation and urbanization expanding impervious surfaces, extreme rain-induced floods are becoming more common ￼. Traditional 2D hydrodynamic models can map floods accurately but are too slow for city-scale risk analyses or ensemble scenarios. Our goal is to develop an AI-driven model that rapidly predicts pluvial flood depth and extent for entire cities, using only terrain data and rainfall input. This model will provide high-resolution flood risk maps for design rainfall scenarios (not real-time forecasts), and will be validated against both simulated flood maps and real-world flood impact data. Below, we outline the data preparation, model architecture, generalization strategies, and validation plan, citing recent advances in AI-for-hydrology research.

Data Sources and Preprocessing

1. Digital Elevation Model (DEM) Processing: We use high-resolution DEMs from the USGS 3D Elevation Program (3DEP) as the primary input. The raw DEM is preprocessed to be hydrologically conditioned: removing spurious sinks or, alternatively, calculating sink depth (the vertical depth of any depression) as a feature input ￼. Key terrain derivatives are extracted as additional input layers, for example: slope, curvature (planform and profile curvature, or mean curvature) ￼, and aspect (encoded as sine/cosine of flow aspect) ￼. These features help the model understand runoff pathways and accumulation tendencies. We also compute flow accumulation or a topographic wetness index, and optionally Height Above Nearest Drainage (HAND) for larger channels ￼, ensuring both small-scale and large-scale drainage characteristics are represented. All raster inputs are aligned and normalized (e.g. min-max scaled). Normalization is applied per local area/patch rather than globally to preserve meaningful relative elevation differences ￼.

2. Rainfall Scenario Definition: Instead of real-time rain fields, we rely on design storm scenarios based on NOAA Atlas 14 statistics. Atlas 14 provides point rainfall frequency estimates (e.g. 5-year or 100-year rainfall depths for durations like 1 hour). From these, we define representative storm intensities for the region of interest. For example, we might use spatially-uniform rainfall hyetographs of 1-hour duration with total depths corresponding to 10-year, 50-year, 100-year return periods, etc ￼ ￼. Multiple temporal hyetograph shapes (intensity time distributions) can be included for each total depth to capture different rainfall burst patterns ￼. Each scenario yields a separate flood map for training/validation. If historical storm events are available, we can also include their recorded total rain volumes to create realistic scenario inputs. In all cases, rainfall is supplied to the model as a simple input vector or map (e.g. a uniform raster layer of total rainfall or a time-series vector fed into the network) rather than a live forecast field.

3. Simulation-Based Flood Map Labels: Because measured pluvial flood depths are scarce, we leverage physics-based models to generate training labels (flood extent and depth maps) for the defined rainfall scenarios ￼. For each city or watershed in the training set, a fast 2D inundation model (e.g. a cellular automata model like WCA2D ￼ or a simplified hydrodynamic model) is run on the DEM to produce water depth at each grid cell ￼. These simulations assume worst-case conditions: e.g. saturated soil and limited stormwater drainage capacity (so that surface topography governs flooding) ￼. By varying rainfall intensities and patterns, we generate a diverse set of flood maps for training. All flood depth rasters are then cropped into patches (e.g. 256 m × 256 m windows) for model input-output pairs, both to increase training samples and to make the learning task tractable in memory ￼ ￼. Each patch includes a border overlap or accompanying lower-resolution context to capture upstream terrain influence (see generalization strategies below). Finally, data is split into training, validation, and hold-out test sets, ensuring that some entire storm events and some locations are kept entirely unseen for unbiased evaluation ￼ ￼.

Model Architecture Considerations

Designing a suitable model requires balancing physical realism, prediction speed, and ability to generalize beyond trained locations. We consider three complementary architecture approaches and recommend an integrated strategy:
	•	Convolutional Neural Network (CNN) Surrogate: CNN-based models treat the DEM and rainfall inputs as image-like grids, learning to output a raster of flood depth. Prior studies show CNN surrogates can emulate 2D flood models orders of magnitude faster than physics simulators ￼. For example, a U-Net style CNN trained on one city predicted water depth maps 1,400× faster than a traditional model with only minor loss in accuracy ￼. We propose using a multi-scale CNN, where the network has parallel inputs: a high-resolution local DEM patch and a coarser but larger-area DEM context. This architecture preserves fine details while giving the model a broader “view” of upstream terrain, improving its predictions of where water will come from ￼ ￼. The CNN can be set up as an encoder–decoder (downsampling the input patch to learn features, then upsampling to output a flood depth map of equal size). We also include the total storm rainfall as a feature – e.g. concatenated as a channel or applied as a scaling factor on the network’s output to enforce sensible scaling (ensuring no flooding if rainfall is zero) ￼ ￼. The CNN approach is purely data-driven but is efficient for large rasterized areas.
	•	Graph-Based Physics-Informed Model: An alternative is to represent the watershed by a graph of hydrologically connected elements (pixels or catchment sub-areas) and use a Graph Neural Network (GNN). Recent work introduced hydraulics-based GNNs that learn flood wave propagation on irregular meshes, maintaining physical constraints like mass conservation ￼. Graph models naturally support transferability to new domains, since they do not depend on a fixed image size and can incorporate invariances like rotation or translation of the coordinate system ￼. For example, a multi-scale shallow-water GNN was able to predict full flood dynamics on unseen terrains and meshes with high accuracy (MAE ~0.05 m) while generalizing to new topographies and even new boundary conditions ￼ ￼. We could construct a graph where nodes represent depressions, channels, or grid cells, and edges represent flow paths derived from the DEM. A GNN can then learn how water propagates from node to node given a rainfall input. Physics-informed techniques (e.g. encoding the shallow water equations in the network or preserving volume balance) would guide the model to physically realistic predictions. This approach is promising for generalization, though more complex to implement; it may require an initial physical simulation (or at least a flow routing pre-computation) to set up the graph structure ￼.
	•	Hybrid and Physics-Informed Neural Networks: To leverage strengths of both fast data-driven models and physical realism, we suggest a hybrid modeling strategy. One option is to run a simplified physical model and let the AI learn a corrective mapping to high-fidelity results ￼. For instance, a coarse-resolution hydrodynamic simulation (with simplified physics) can produce a rough flood estimate quickly, and a neural network then refines this into a high-resolution depth map ￼. Fraehr et al. (2023) followed this approach, combining a coarse 2D model with a learned transformation to produce near-instant flood maps matching a full 2D solver’s accuracy ￼. Another approach is to embed known physics into the neural network architecture or loss function (so-called physics-informed neural networks). In our context, we can constrain the model to obey mass balance (volume in = volume out + storage) during training or penalize physically implausible behaviors. We can also use dimensionless input features derived from hydrologic equations (e.g. ratios of rainfall intensity to slope or storage capacity) to inform the model. By incorporating the Buckingham π theorem, one can derive dimensionless groups that govern flooding and use them as inputs, which has been shown to improve model interpretability and transferability ￼. Overall, a hybrid model would exploit the speed of ML while anchoring its outputs in physical reality, which is crucial for trust in unseen locations.

Strategies for Generalization to Unseen Watersheds

A core requirement is that the model perform well on cities or watersheds it was not trained on. We adopt several strategies to improve generalization:
	•	Multi-Scale Spatial Context: A small DEM patch alone can cause edge-effects and missed upstream contributions when applied to a new area. We expand the model’s receptive field by providing larger-scale context. In our CNN approach, this means inputting not just a 256×256 m high-res patch, but also a surrounding DEM at lower resolution (e.g. 512×512 m area at 2 m resolution, and perhaps 1024×1024 m at 4 m resolution) ￼ ￼. Cache et al. (2024) showed that including terrain context in this way significantly improved the identification of flood hotspots in new cities compared to a strictly patch-based model ￼ ￼. The model learns both local depressions and the broader slope/channel structure feeding into them, making it less likely to overfit to one site’s idiosyncrasies.
	•	Physical Feature Augmentation: We incorporate physically meaningful features as model inputs across all regions. By using descriptors like slope, curvature, flow accumulation, and dimensionless indices, the model bases its decisions on fundamental drivers of flooding rather than arbitrary pixel values. Recent research demonstrated that using dimensionless, multi-scale features (derived from first principles) can enable an ML model trained in one region to perform well in a completely different region ￼. In that study, a logistic regression using features constrained by the Buckingham π theorem outperformed models using raw dimensional features, especially in cross-region tests ￼. This suggests our model will generalize better if we feed it inputs like scaled catchment area, slope–rainfall ratios, etc., which capture the similarity of runoff processes across landscapes ￼ ￼. Such features act as common denominators, helping the model recognize, for example, that “a 2% slope with 50 mm rain” in one city is analogous (in terms of flooding propensity) to “a 1% slope with 25 mm rain” in another after nondimensionalization.
	•	Diverse Training Dataset: We will train on a wide range of terrains and rainfall events so the model learns general patterns. This includes using multiple cities’ data if available, or generating synthetic variations of one city’s DEM (e.g. warping slopes, adding/removing certain features) to augment the training set. Guo et al. (2022) demonstrated a model could adapt to 656 different catchments when care was taken in training, though in their case a single fixed rainfall event was used for all ￼. We improve on that by exposing our model to diverse storm intensities and durations as well, ensuring it sees both frequent minor floods and extreme rare floods during training. We also prevent overfitting by using techniques like random rotation or flipping of patches (to remove directional biases) and leave-one-area-out validation, where entire neighborhoods or cities are withheld to test generalization.
	•	Transfer Learning for New Locations: To efficiently adapt the model to an entirely new city, we can employ transfer learning. Instead of training from scratch, we start with the model pre-trained on our broad dataset and fine-tune it using a small amount of data from the new city ￼ ￼. Notably, Cache et al. (2024) found that by doing this, their flood model could be successfully adapted to a new city using only a single rainfall event’s data, while still remaining robust across different rainfall conditions ￼ ￼. This means we could take our model trained on, say, City A and City B, then update it with just one known flood scenario from City C to achieve good results in City C. Transfer learning, along with fine-tuning hyperparameters, will be part of our generalization toolkit.
	•	Invariant Learning and Regularization: We design the model to respect certain invariances so it doesn’t latch onto location-specific values. For instance, we remove any absolute position or coordinate information from inputs, aside from relative heights (the model doesn’t need to know an elevation is “100 m above sea level,” only the relative heights in the area). We also normalize features per local patch as noted, which avoids the situation where different cities’ elevation ranges confuse the model ￼. Regularization techniques like dropout and weight decay will be applied during training to avoid overfitting to noise. Moreover, we can impose physical regularization – e.g. adding a penalty if the total volume of predicted water in a basin deviates greatly from the volume of rainfall input – to encourage physically consistent behavior even on unseen terrain.

Validation and Use of Flood Loss Claim Data

We will validate the AI model against both historical flood maps and insurance loss data to ensure it is capturing real flood behavior:
	•	Validation with Historical Flood Maps: Whenever available, we will compare the model’s predicted inundation extents and depths to observed flood data from past events. This could include high-water marks and surveyed flood depths (for example, post-event high-water marks from Hurricane Harvey were used to validate a recent flood model ￼), or remote sensing-derived flood extent maps. A close match in flood extent (measured by metrics like Intersection-over-Union or Critical Success Index) and in depth accuracy (low mean absolute error in high-water locations) will indicate the model is reproducing real flood patterns. In absence of extensive observations, we use high-fidelity 2D model results as a benchmark. For instance, Samadi et al. (2024) compared their DEM-based model to both field measurements and a physics-based model’s output to build confidence in performance ￼ ￼. We will adopt a similar approach: the AI’s outputs on test scenarios are checked against a trusted hydrodynamic model’s results that were not used in training.
	•	Using Insurance Claims for Validation: A novel aspect of our validation is leveraging the NFIP flood insurance claims data for pluvial flood events. Recent research compiled nationwide NFIP claims (1978–2021) and identified that the vast majority of claims outside of FEMA floodplains (87.1%) were attributable to pluvial flooding ￼. These claim records (as analyzed by Nelson-Mercer et al., 2025) provide spatial and temporal markers of past pluvial flood impacts. We will use this data in several ways to validate the model:
	•	Spatial validation: We will check if the model’s predicted high-risk flood zones correspond to areas where many insurance claims have occurred historically. If a neighborhood is repeatedly predicted to have deep ponding under heavy rain, we expect to see a cluster of pluvial flood claims there over the past decades. Agreement would bolster confidence that the model’s risk maps are meaningful. Discrepancies might reveal missing factors (e.g. drainage infrastructure not in the DEM) or could indicate under-reporting of claims in some areas.
	•	Event-specific validation: For significant past storm events that caused documented urban flooding, we can simulate those events in our model and compare predicted outcomes to the actual claim occurrences. For example, if on a certain date a city experienced a cloudburst and we know which properties filed flood claims, we run the model with that event’s rainfall and verify that it predicts flooding at those locations. A successful model should flag those spots (e.g. >0.3 m water depth) whereas it should not predict false floods in nearby areas that stayed dry. This temporal alignment validates that the model responds correctly to real rainfall extremes.
	•	Intensity of losses: The insurance data also includes information on damage severity (e.g. payout amounts or percent damage to the property) ￼. While not a direct measurement of water depth, a higher claim payout often correlates with deeper or more severe flooding. We can explore whether our predicted depth at a claim location correlates with the claim’s damage percent. For instance, the median damage for pluvial claims was found to be about 9.4% of property value (much lower than riverine floods) ￼, implying many pluvial events are shallow inundations. Our model’s depth predictions for those claim events should likewise mostly be in the lower range (e.g. a few tens of centimeters), aligning with the relatively moderate damage. Such comparisons provide a reality-check on the model’s output magnitude.

All validations will be quantified. We will calculate flood extent overlap with observations, depth error statistics, and use the claims dataset to compute, for example, the hit rate: what fraction of claim locations are correctly predicted as flooded by our model versus false alarm rate (predicted flood where no claim/history exists). By validating not just on hydraulic models but also on real-world impact data, we ensure the AI model’s maps are trustworthy for practical risk management. Notably, the claims-based evaluation helps capture aspects of flooding that models alone might miss – for example, if our model underestimates flooding in a region that actually had many insurance claims, it signals a need to incorporate additional factors or recalibrate in that region ￼. This iterative validation with historical data will guide refinements to the model.

Conclusion

In summary, we propose an AI-driven approach to city-scale pluvial flood mapping that integrates detailed terrain data, design rainfall scenarios, and state-of-the-art modeling techniques. The approach emphasizes generalization and physical realism: using multi-scale CNN or GNN architectures informed by hydrologic features, plus training across diverse conditions to avoid overfitting to any one locale. Rigor in data processing – from DEM conditioning to using Atlas 14 rainfall – ensures the inputs reflect true flood drivers. A combination of simulation-based training data and real-world validation (flood maps and insurance claims) will produce a model that not only fits the training cases but also reliably predicts flood hazard in new cities and under future extreme rain events. By citing recent advances in AI for hydrology and leveraging unique datasets like NFIP pluvial claims, our approach builds on the cutting edge of research ￼ ￼. The result will be a robust tool for urban flood risk assessment, enabling planners and engineers to identify vulnerable areas and take mitigation actions before the next big storm hits.

Sources: Recent literature on data-driven flood modeling, including: Nelson-Mercer et al., 2025 ￼; Cache et al., 2024 ￼ ￼; Guo et al., 2021 ￼; Guo et al., 2022 ￼; Bartlett et al., 2024 ￼; Fraehr et al., 2023 ￼; Bentivoglio et al., 2025 ￼; Samadi et al., 2024 ￼. Each of these has informed the recommended methodology and is cited inline above for further reading on specific techniques and findings.
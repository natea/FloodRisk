Accelerating Pluvial Flood Modeling with Existing CNN Architectures

Overview of Relevant Models in Existing Repositories

UNOSAT Rapid Mapping (Repo 1 – Flood Segmentation U-Net): The UNOSAT repository provides a fully convolutional U-Net model (implemented with Fastai) for segmenting flooded areas in satellite SAR images ￼. This model uses an encoder-decoder architecture with a ResNet-34 backbone pretrained on ImageNet (for the encoder) and skip connections for high-resolution segmentation ￼. The U-Net takes an image (e.g. Sentinel-1 SAR data) as input and outputs a pixel-wise flood mask. Notably, the training setup included ImageNet normalization, extensive data augmentation (flips, rotations, scale, etc.), and a weighted cross-entropy loss to address class imbalance (since flood pixels are relatively rare) ￼. This yielded strong performance (e.g. ~0.92 Dice coefficient and ~0.91–0.92 recall in their tests) ￼. In summary, Repo 1’s model is a classic U-Net segmentation network with a pretrained CNN encoder and has proven effective for identifying flooded pixels.

Rapid_FloodModelling_CNN (Repo 2 – Fluvial Flood Depth CNN): The second repository implements a different approach: a custom CNN that directly predicts flood water depths across a domain, trained as a surrogate to a hydraulic model ￼. The architecture (built in Keras) is relatively simple: it uses two convolutional layers followed by three dense (fully-connected) layers ￼. Uniquely, the output layer has one node per grid cell in the simulation domain (e.g. 581,061 output nodes for their case study) ￼. In other words, the model ingests certain 1D features and produces an entire flood depth map as a flattened output vector. The input to this CNN is not an image but rather hand-crafted features: specifically, time-series data of upstream flow hydrographs (with lagged timesteps) that drive the flood ￼. Thus, this is essentially a 1D temporal CNN that learns the mapping from boundary conditions (river inflows) to the spatial distribution of water depths. The authors tuned hyperparameters via Bayesian optimization; the final model used 32 and 128 filters in the conv layers, dense layers of size 32, 256, and 512, and was trained with the Adam optimizer (batch size 10) ￼. No pretrained weights were used (the model was trained from scratch on simulated data), and it likely optimized a regression loss (mean squared error) to fit the continuous depth values. This CNN achieved high accuracy in reproducing the 2D model outputs (errors <0.3 m for ~97% of cells in their test scenario) ￼ ￼. In summary, Repo 2’s model is a lightweight, fully-connected CNN surrogate tailored to a specific watershed and input format (upstream rain/runoff signals). It is not an image-based segmentation network, but rather a specialized regression CNN that outputs flood depth for a fixed domain.

Portability of Components to a DEM+Rainfall Flood Model

When developing an AI model for pluvial flooding (predicting flood extent or depth from DEM and rainfall inputs), several components of the above models can be leveraged, while others require modification:
	•	Model Architecture (Encoder–Decoder vs. Fully-Connected CNN): The U-Net architecture from Repo 1 is directly portable to our task. It is well-suited for image-to-image mapping, so we can feed a multi-channel raster (e.g. DEM and rainfall intensity maps) into the U-Net and have it output a flood inundation map. This architecture can inherently capture spatial relationships (e.g. downhill flow paths) thanks to convolutional filters and skip connections, which is ideal for mapping terrain + rainfall to flood extent. In contrast, the Repo 2 CNN would require substantial modification to handle DEM+rainfall inputs. That model currently takes only aggregated 1D features (river hydrographs) – it has no mechanism to directly ingest a 2D DEM or spatial rainfall distribution. Adapting it would mean redesigning it into a 2D CNN (or a hybrid model) that processes gridded inputs. Essentially, one would end up moving toward an encoder-decoder architecture anyway. Therefore, reusing the U-Net/FCN approach from Repo 1 is the more natural choice for a DEM+rainfall model, whereas Repo 2’s fully-connected output approach is too specialized to its original use case to apply off-the-shelf.
	•	Pretrained Weights and Transfer Learning: Repo 1’s use of a pretrained ResNet-34 backbone is a valuable feature we can reuse. Those ImageNet-trained weights provide a strong starting point for feature extraction, even though our input channels (DEM, rainfall) are not standard images ￼. In practice, we can initialize the U-Net encoder with ResNet weights and then fine-tune it on our flood data. (We will need to adjust the first convolution to accept 2 input channels instead of 3; this can be done by replicating or averaging weights from the pretrained RGB channels, or simply initializing a new 2-channel conv layer.) Using transfer learning should speed up convergence and potentially improve accuracy, given limited flood training data. On the other hand, Repo 2’s model did not rely on any pretrained model – it learned from scratch on simulated floods. Its learned weights are specific to that one region (since the model effectively “baked in” the Carlisle area topography via training). Those weights are not directly usable for a new city’s DEM. Thus, the main transfer learning opportunity comes from Repo 1’s pretrained encoder, not from Repo 2. We should, however, ensure that input data for the pretrained model is normalized similarly to what it was trained on (Repo 1 applied ImageNet normalization to the input ￼). For DEM and rainfall, this means scaling/normalizing these layers to a comparable range so the pretrained features still make sense, or otherwise be prepared to fine-tune the early layers more heavily.
	•	Training Pipeline and Data Handling: Both repositories emphasize training on simulated or annotated data, which aligns with our needs. From Repo 1, we can adopt the idea of tiling the input data into manageable patches and excluding non-informative samples. They generated 256×256 image chips and discarded tiles with no flood pixels to avoid wasting capacity on all-background images ￼. For pluvial flooding, this is analogous to focusing training on areas and times where flooding actually occurs. We can similarly tile a city’s DEM and rainfall map into patches for training, and oversample regions or storm events that produce flooding (skipping patches where there is no inundation). Repo 1 also used extensive data augmentation (random rotations, flips, warping) to increase robustness ￼. We should reuse these augmentation techniques cautiously – flips or rotations of DEM/rainfall fields are generally acceptable and can help the model generalize, so long as the physics isn’t overly violated (e.g. flipping won’t change downhill directions fundamentally on an isotropic terrain grid). Repo 2’s training pipeline involved generating a large synthetic dataset of flood scenarios using a physics-based model (LISFLOOD-FP) ￼ ￼. This is a valuable concept: we can similarly run a hydrodynamic model on many hypothetical rainfall events over the city’s DEM to create a training set of input (rain, DEM) and output (flood depth) pairs. In short, using simulation data for training and validation – as done in Repo 2 – is directly applicable to our problem and will accelerate model development by providing plentiful labeled examples. The code specifics from Repo 2 (written in Keras) are less important to carry over than the methodology of data generation and splitting (e.g. training on synthetic events, testing on real events).
	•	Components and Utilities: We can extract several useful components from these projects:
	•	Architecture Implementation: The encoder-decoder network definition from Repo 1 can be reused. For example, using Fastai’s unet_learner with a ResNet-34 backbone (as they did) provides a quick way to get a working model ￼ ￼. Alternatively, we can implement a similar U-Net in PyTorch using their approach as a template.
	•	Loss Functions: If we aim to predict a binary flood extent map, we should adopt a weighted loss function like Repo 1 did (to handle the imbalance between flooded and dry pixels) ￼. They used weighted cross-entropy; we might also consider Dice or IoU loss as additional metrics, given the segmentation nature. If our model predicts continuous flood depths, we will switch to a regression loss (mean squared error or mean absolute error). Repo 2 likely trained with MSE loss (since it predicts water depth values); this would be the natural choice for a depth regression model. In fact, we might design the network to output both a flood mask and depths (e.g. a multitask approach), but a simpler route is to predict depth and treat non-flooded areas as zero depth in the target. In that case, using an MSE loss over all pixels (where the model can learn to output near-zero for most and positive depths for ponded areas) works, though we may still weight or mask the loss so that learning isn’t dominated by the vast majority of zero-depth cells.
	•	Evaluation Metrics: Repo 1 reported Dice coefficient, accuracy, precision/recall ￼ – we should similarly monitor metrics like IoU or F1-score for extent, and RMSE for depth, to gauge model performance. Repo 2 demonstrated very low error distributions (e.g. <0.2 m error for most cells) ￼; we can use such statistics as a benchmark for our model’s accuracy.
	•	Checkpointing & Inference: There’s nothing particularly novel about how the models are saved or loaded in these repos – we can rely on standard PyTorch/Fastai checkpointing. However, Repo 2’s approach of outputting a full domain in one go does mean the model is tied to a specific grid size. In our U-Net approach, the model can in principle handle variable image sizes (within memory limits), which is more flexible – we could train on smaller patches and still deploy on a full-size DEM if needed by tiling or sliding window inference.

Required Modifications and Tuning for DEM + Rainfall Application

To adapt and optimize these models for a pluvial flooding task (city-scale, using DEM and rainfall as inputs), we need to apply several modifications:
	•	Input Channel Modification: The UNOSAT U-Net expects a single-channel SAR image (or generally, 3-channel RGB if using a typical ResNet pretrained on RGB). We will modify the input to have 2 channels: [DEM, Rainfall]. This means adjusting the first convolutional layer of the network. If using the ResNet-34 backbone, we can initialize the first layer’s weights appropriately (e.g., copy the single-channel kernel or average the RGB kernels to the two channels) or simply initialize randomly and let it learn. After this change, the rest of the U-Net architecture can remain the same since it’s fully convolutional. The network will learn filters that combine elevation and rainfall features to detect where water would accumulate.
	•	Output and Activation: We must decide on the output representation. For flood extent (binary classification), the U-Net’s final layer should have 1 channel with a sigmoid activation, and we’d train with a classification loss (as in Repo 1). For flood depth (regression), the final layer should produce one channel of continuous values. In that case, we would use a linear output (no activation) and train with an MSE loss. It may be beneficial to output depth directly; any pixel with predicted depth > 0 can be considered flooded, thereby giving us both extent and depth from one model. This is a change from Repo 1 (which was purely classification) – we’ll be fine-tuning the loss function and possibly the network output activation to suit regression. We should also consider applying a clamp or appropriate activation (e.g. ReLU) to ensure non-negative outputs, since negative flood depths are non-physical. A ReLU on the final layer could enforce zero-or-positive depth predictions.
	•	Training Hyperparameters: We should reuse sensible hyperparams from the repos but remain ready to fine-tune them for our data:
	•	Backbone Depth: Starting with ResNet-34 (as in Repo 1) is reasonable ￼, given its success. We might experiment with a deeper backbone (ResNet-50) if the dataset is large enough, but deeper models increase training time and risk overfitting. Jeremy Howard (Fastai) and others suggested that ResNet-50 could improve results if handled properly ￼, but the ResNet-34 was already quite effective for the flood mapping task ￼.
	•	Optimizer and LR Schedule: Both studies used Adam (Repo 2 explicitly, Repo 1 likely via Fastai’s 1-cycle policy with Adam) ￼. We can stick with Adam or AdamW. Fastai’s 1-cycle learning rate schedule proved robust in Repo 1, so using a one-cycle policy or a cosine annealing schedule would be a good starting point. We will tune the learning rate range by doing a small learning rate finder experiment (a common Fastai practice) before full training.
	•	Batch Size: Repo 2 used a batch size of 10 (likely constrained by the huge output vector) ￼. With a U-Net on 256×256 patches, we might handle larger batch sizes (depending on GPU memory). A batch of 8–16 could be feasible; we will adjust based on memory usage. Larger batch sizes can be used if we accumulate gradients or use smaller patch sizes. The key is to ensure each batch has a good mix of flooded vs unflooded areas to prevent bias.
	•	Hyperparameter Tuning: Repo 2 employed Bayesian hyperparameter search to optimize filter counts and neuron counts ￼. For our U-Net, many architectural hyperparams (filter sizes, etc.) are predefined by the ResNet backbone and U-Net design. We might not need an extensive search, but we should be open to tuning certain things like the weighting of the loss (if using weighted loss, what class weight to use for flood vs non-flood), the data augmentation parameters, and possibly the tile size or overlap. If time permits, a small grid search or Bayesian optimization on learning rate, weight decay, and dropout could be useful to squeeze out extra performance.
	•	Incorporating Rainfall Temporal Dynamics: Pluvial flooding depends on not just total rainfall volume but also the temporal distribution (intensity over time). Neither repo explicitly handled spatiotemporal input in the model – Repo 1 dealt with a single static image, and Repo 2 incorporated time by feeding multiple timesteps of discharge into a 1D CNN ￼. If our application needs to consider a rainfall hyetograph over time, we may need to extend the model. One approach is to encode time as additional input channels (e.g. feed cumulative rainfall at different time intervals as separate raster layers). Another approach is a two-branch model: one branch is a CNN that processes the static DEM (and perhaps soil/land cover if available), and another branch is an RNN or temporal CNN that processes the rainfall time-series, and then merges with the spatial branch. Designing such a multi-headed model would be a significant modification, essentially combining ideas from Repo 2 (time-series handling) with Repo 1 (spatial U-Net) ￼ ￼. This is an advanced extension – if the goal is a rapid prototype, one might simplify by considering a worst-case or total rainfall input rather than the full time series initially. For now, if we assume the input “rainfall” can be represented as a single aggregate map (e.g. total rainfall or peak intensity map), the standard U-Net can handle it directly as noted. If later the temporal aspect must be added, we can draw on techniques similar to Repo 2 (e.g. feeding in multiple time-step features, or using a sequence model).
	•	Physical Consistency and Post-Processing: It’s worth noting that purely data-driven models may produce physically implausible artifacts (e.g. isolated flooded pixels in high areas). Repo 1’s output is a binary mask and sometimes such models benefit from morphological post-processing to clean noise. Repo 2’s output was continuous and presumably smoother due to learning from a physical model. For our case, we might implement some constraints or post-processing – for example, ensure connectivity of flooded areas or impose that water shouldn’t appear on hilltops without a source. These are not components in the provided repos, but a consideration for deployment. Using the DEM as an input will help the model learn that only depressions fill with water, but some additional regularization (maybe via the loss function penalizing unrealistic patterns) could be beneficial. This goes beyond direct code reuse, but it’s an area where domain knowledge should guide model fine-tuning.

Recommended Approach for the DEM+Rainfall Flood Model

Leverage U-Net Architecture: Overall, the U-Net from Repo 1 offers a powerful and adaptable framework for our pluvial flood prediction goal. We should reuse its encoder-decoder design, initialize with pretrained weights for faster convergence, and incorporate our DEM and rainfall data as input layers. This model’s ability to learn spatial features (like drainage pathways, basin areas) aligns well with how rainfall-runoff patterns manifest on a DEM. By training it on simulated rainfall-flood pairs, we expect it to learn to produce flood extent/depth maps that closely match the simulation outputs – much like Repo 1’s model learned to mimic expert-derived flood maps, and Repo 2’s model emulated a physics model’s output.

Reuse Training Techniques: We will follow training best practices demonstrated in the repos: apply data augmentation to improve generalization (as in Repo 1) ￼, and generate a broad training dataset covering diverse storm scenarios (as in Repo 2) ￼ ￼. Transfer learning (pretrained ResNet encoder) will be used initially, but we’ll monitor if the model needs to “unlearn” any irrelevant features due to the non-natural image input. If so, we can allow more epochs or lower learning rates on the earlier layers. We will also carry over the idea of weighting the loss function or balancing the training data so that flooded pixels (which might be only a small fraction of the map in most cases) are learned effectively ￼.

Fine-Tune Hyperparameters: Key hyperparameters (learning rate, batch size, etc.) will be tuned to our specific data. As a starting point, using Adam with a moderate learning rate (e.g. 1e-3 with cosine annealing) and a batch size that fits our hardware is reasonable. We’ll use early stopping or model checkpointing to avoid overfitting. If the model struggles to converge or generalize, we may revisit architecture (for example, add dropout layers or try a different backbone). However, given the prior successes – 0.91+ Dice with ResNet-34 U-Net on a similar segmentation task ￼, and low errors from a relatively small CNN on flood regression ￼ – we expect that a ResNet-based U-Net fine-tuned to our data will achieve state-of-the-art results for predicting pluvial flood extents and depths.

In summary, most components from Repo 1 can be directly reused with minimal changes (the U-Net model, training strategy, and even the pretrained weights), making it a strong foundation for the DEM+rainfall model. Components from Repo 2 are conceptually useful but would need re-thinking – for example, using simulation-generated training data and considering how to input time-series rainfall – rather than direct code reuse of its architecture. By combining the robust architecture of the UNOSAT U-Net ￼ with the data-driven philosophy of the CNN surrogate model ￼ (training on physics-based flood simulations), we can rapidly develop a pluvial flood model that is both accurate and efficient for city-scale flood prediction. All critical architecture choices and hyperparameters will be validated and tuned on our specific dataset, but the experience from these two repositories provides a clear roadmap for what works and how to adapt it to our needs.

Sources:
	•	Nemni et al. (2020), “Fully Convolutional Neural Network for Rapid Flood Segmentation in SAR Imagery” – UNOSAT FloodAI model (Fastai U-Net) ￼ ￼
	•	Kabir et al. (2020), “Deep CNN for Rapid Prediction of Fluvial Flood Inundation” – CNN surrogate model (Keras sequential conv+dense) ￼ ￼
	•	Both repositories on GitHub (UNITAR-UNOSAT and SRKabir) for implementation details and training techniques.
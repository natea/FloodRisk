# FloodRisk Validation Framework

A comprehensive data validation and quality assurance system for the FloodRisk ML pipeline, ensuring end-to-end data quality from acquisition through ML training and deployment.

## üéØ Overview

The FloodRisk Validation Framework provides:

- **End-to-End Pipeline Validation**: Comprehensive quality checks across all pipeline stages
- **Spatial Data Validation**: CRS consistency, extent alignment, resolution matching
- **ML Integration Validation**: Dataset compatibility, model performance, label quality
- **Automated QA Reporting**: Interactive dashboards, alerts, and trend analysis
- **Real-time Monitoring**: Performance tracking and quality degradation detection

## üìÅ Framework Structure

```
src/validation/
‚îú‚îÄ‚îÄ __init__.py                    # Main framework exports
‚îú‚îÄ‚îÄ pipeline_validator.py         # Core pipeline validation components
‚îú‚îÄ‚îÄ ml_integration_validator.py   # ML pipeline integration validation
‚îú‚îÄ‚îÄ qa_dashboard.py               # Automated QA reporting and dashboards  
‚îú‚îÄ‚îÄ metrics.py                    # Validation metrics (existing)
‚îú‚îÄ‚îÄ visualization.py              # Validation visualizations (existing)
‚îî‚îÄ‚îÄ report_generator.py           # Report generation (existing)

tests/validation/
‚îú‚îÄ‚îÄ test_pipeline_validator.py    # Pipeline validation tests
‚îú‚îÄ‚îÄ test_ml_integration.py        # ML integration tests
‚îî‚îÄ‚îÄ test_qa_dashboard.py          # QA dashboard tests

examples/
‚îú‚îÄ‚îÄ validation_example.py         # Comprehensive usage examples
‚îî‚îÄ‚îÄ quick_start.py                # Quick start guide
```

## üöÄ Quick Start

### Basic Pipeline Validation

```python
from src.validation.pipeline_validator import PipelineValidator

# Initialize validator
validator = PipelineValidator()

# Prepare your data
pipeline_data = {
    'dem_path': 'path/to/dem.tif',
    'rainfall_data': rainfall_array,  # numpy array or path
    'simulation_results': {
        'depths': depth_array,
        'convergence': convergence_info
    },
    'tiles_info': {
        'tiles': tile_data_list,
        'metadata': tile_metadata
    }
}

# Run validation
results = validator.validate_full_pipeline(pipeline_data)

# Check results
for component, result in results.items():
    print(f"{component}: {result.status} (Score: {result.score:.3f})")
    if result.issues:
        print(f"  Issues: {result.issues}")
```

### ML Integration Validation

```python
from src.validation.ml_integration_validator import MLIntegrationValidator

# Initialize ML validator
ml_validator = MLIntegrationValidator()

# Prepare ML data
ml_data = {
    'dataset': your_pytorch_dataset,
    'labels_data': {'labels': training_labels},
    'model_results': {
        'training_history': training_metrics,
        'test_metrics': test_performance,
        'real_data_metrics': real_data_results,
        'dummy_data_metrics': dummy_data_results
    }
}

# Run ML validation
ml_results = ml_validator.validate_ml_pipeline(ml_data)

# Get overall ML pipeline status
status = ml_validator.get_ml_pipeline_status()
print(f"ML Pipeline Status: {status}")
```

### Automated QA Dashboard

```python
from src.validation.qa_dashboard import QADashboard

# Initialize dashboard with persistent storage
dashboard = QADashboard("validation_history.db")

# Process validation results
dashboard_result = dashboard.process_validation_results(
    validation_results,
    pipeline_type="FloodRisk ML Pipeline",
    generate_report=True,
    report_path="qa_report.html"
)

# Get dashboard data for monitoring
dashboard_data = dashboard.get_dashboard_data(days=30)
```

## üîç Validation Components

### 1. DEM Quality Validation

Validates digital elevation model quality:

- **Elevation Range**: Checks for realistic elevation values
- **Void Detection**: Identifies and quantifies missing data
- **Spatial Continuity**: Detects unrealistic elevation gradients
- **CRS Validation**: Ensures valid coordinate reference system

```python
from src.validation.pipeline_validator import DEMValidator

dem_validator = DEMValidator({
    'elevation_bounds': (-500, 9000),  # meters
    'void_threshold': 0.05,            # 5% max voids
    'smoothness_threshold': 100        # max elevation change per pixel
})

result = dem_validator.validate('path/to/dem.tif')
```

### 2. Rainfall Data Validation

Validates precipitation data quality:

- **Value Range**: Checks for realistic rainfall intensities
- **Spatial Coverage**: Ensures adequate spatial coverage
- **Temporal Consistency**: Validates temporal patterns (if applicable)
- **Missing Data Analysis**: Quantifies data gaps

```python
from src.validation.pipeline_validator import RainfallValidator

rainfall_validator = RainfallValidator({
    'max_intensity': 500,              # mm/hr
    'min_coverage': 0.95,              # 95% spatial coverage
    'missing_data_threshold': 0.1      # 10% max missing
})

result = rainfall_validator.validate(rainfall_array)
```

### 3. Spatial Consistency Validation

Ensures spatial alignment across datasets:

- **CRS Consistency**: Validates coordinate systems match
- **Extent Alignment**: Checks spatial overlap between datasets
- **Resolution Matching**: Ensures compatible resolutions
- **Projection Accuracy**: Validates projection parameters

```python
from src.validation.pipeline_validator import SpatialConsistencyValidator

spatial_validator = SpatialConsistencyValidator({
    'spatial_tolerance': 0.1,          # tolerance in coordinate units
    'min_overlap': 0.95                # 95% minimum overlap required
})

datasets = [
    {'path': 'dem.tif', 'name': 'DEM', 'type': 'elevation'},
    {'path': 'rainfall.tif', 'name': 'Rainfall', 'type': 'precipitation'}
]

result = spatial_validator.validate(datasets)
```

### 4. Simulation Results Validation

Validates flood simulation outputs:

- **Physical Plausibility**: Checks for realistic flood depths and velocities
- **Mass Conservation**: Validates water balance
- **Convergence Analysis**: Ensures simulation convergence
- **Boundary Conditions**: Validates boundary condition consistency

```python
from src.validation.pipeline_validator import SimulationValidator

sim_validator = SimulationValidator({
    'max_depth': 50.0,                 # 50m max flood depth
    'mass_conservation_tolerance': 0.05, # 5% mass balance tolerance
    'convergence_threshold': 1e-6       # convergence criteria
})

simulation_results = {
    'depths': flood_depths,
    'velocities': velocity_field,
    'convergence': convergence_info,
    'inflow': inflow_rate,
    'outflow': outflow_rate
}

result = sim_validator.validate(simulation_results)
```

### 5. Tile Quality Validation

Validates ML training tile generation:

- **Flood/Dry Balance**: Ensures appropriate class distribution
- **Spatial Coverage**: Validates geographic coverage
- **Edge Effects**: Detects edge artifacts
- **Size Consistency**: Ensures uniform tile dimensions

```python
from src.validation.pipeline_validator import TileQualityValidator

tile_validator = TileQualityValidator({
    'target_flood_ratio': (0.1, 0.9),  # 10-90% flood coverage
    'min_tiles': 100,                   # minimum number of tiles
    'edge_threshold': 5                 # edge effect detection threshold
})

tiles_info = {
    'tiles': list_of_tile_data,
    'metadata': {
        'tile_size': (256, 256),
        'overlap': 0.1,
        'projection': 'EPSG:4326'
    }
}

result = tile_validator.validate(tiles_info)
```

### 6. ML Data Compatibility Validation

Validates ML dataset format and compatibility:

- **Tensor Shapes**: Ensures correct input/output dimensions
- **Data Types**: Validates appropriate data types for training
- **Memory Requirements**: Estimates memory usage
- **DataLoader Compatibility**: Tests PyTorch DataLoader integration

### 7. Model Performance Validation

Validates ML model training and performance:

- **Training Convergence**: Analyzes training curves
- **Overfitting Detection**: Identifies train/validation gaps
- **Performance Metrics**: Validates accuracy, precision, recall
- **Inference Speed**: Measures prediction performance
- **Real vs Dummy Data**: Compares performance on real vs synthetic data

### 8. Label Quality Validation

Validates training label quality:

- **Class Balance**: Analyzes class distribution
- **Spatial Coherence**: Validates spatial consistency
- **Label Noise**: Detects potential labeling errors
- **Value Range**: Ensures appropriate label values

## üìä QA Dashboard Features

### Automated Reporting

The QA dashboard generates comprehensive HTML reports with:

- **Executive Summary**: Overall validation status and scores
- **Component Details**: Individual validation results
- **Visualizations**: Charts, plots, and trend analysis
- **Interactive Dashboards**: Plotly-based interactive visualizations
- **Recommendations**: Actionable improvement suggestions

### Real-time Monitoring

- **Validation History**: Track validation performance over time
- **Trend Analysis**: Detect performance degradation
- **Alert System**: Automated notifications for quality issues
- **Performance Metrics**: Monitor validation speeds and resource usage

### Database Storage

Persistent storage of validation results enables:

- **Historical Analysis**: Compare validation results over time
- **Trend Detection**: Identify patterns and degradation
- **Alert Management**: Track and resolve quality issues
- **Report Generation**: Create comprehensive validation reports

## üß™ Testing

Comprehensive test suites ensure framework reliability:

```bash
# Run all validation tests
pytest tests/validation/ -v

# Run specific test modules
pytest tests/validation/test_pipeline_validator.py -v
pytest tests/validation/test_ml_integration.py -v

# Run with coverage
pytest tests/validation/ --cov=src.validation --cov-report=html
```

### Test Categories

- **Unit Tests**: Individual validator components
- **Integration Tests**: Complete pipeline workflows
- **Performance Tests**: Validation speed and resource usage
- **Edge Case Tests**: Error handling and boundary conditions

## üìà Performance Optimization

### Validation Speed

- **Parallel Processing**: Concurrent validation of multiple components
- **Sampling**: Statistical sampling for large datasets
- **Caching**: Cache validation results for repeated analyses
- **Early Termination**: Stop validation on critical failures

### Memory Management

- **Chunk Processing**: Process large datasets in chunks
- **Memory Monitoring**: Track and limit memory usage
- **Garbage Collection**: Explicit memory cleanup
- **Efficient Data Structures**: Optimized data handling

## üîß Configuration

### Validator Configuration

```python
validation_config = {
    'dem': {
        'elevation_bounds': (-100, 3000),
        'void_threshold': 0.05,
        'smoothness_threshold': 100
    },
    'rainfall': {
        'max_intensity': 200,
        'min_coverage': 0.9,
        'missing_data_threshold': 0.1
    },
    'spatial': {
        'spatial_tolerance': 0.1,
        'min_overlap': 0.95
    },
    'simulation': {
        'max_depth': 20.0,
        'mass_conservation_tolerance': 0.05,
        'convergence_threshold': 1e-6
    },
    'tiles': {
        'target_flood_ratio': (0.1, 0.9),
        'min_tiles': 100,
        'edge_threshold': 5
    },
    'ml_data': {
        'expected_input_shape': (3, 256, 256),
        'expected_output_shape': (1, 256, 256),
        'batch_size': 32,
        'max_memory_gb': 16
    },
    'model_performance': {
        'min_accuracy': 0.7,
        'max_overfitting_gap': 0.1,
        'inference_speed_threshold': 100  # ms
    },
    'label_quality': {
        'min_class_ratio': 0.01,
        'spatial_coherence_threshold': 0.8
    }
}
```

### Dashboard Configuration

```python
dashboard_config = {
    'database_path': 'validation_history.db',
    'alert_thresholds': {
        'critical_score': 0.6,
        'warning_score': 0.8,
        'trend_degradation': 0.1
    },
    'report_settings': {
        'include_visualizations': True,
        'generate_pdf': False,
        'max_history_days': 90
    },
    'visualization': {
        'style': 'seaborn-v0_8',
        'color_palette': 'husl',
        'figure_size': (12, 8)
    }
}
```

## üö® Alert System

The validation framework includes an automated alert system:

### Alert Types

- **CRITICAL**: Score below critical threshold (default: 0.6)
- **WARNING**: Score below warning threshold (default: 0.8)
- **TREND**: Consistent performance degradation detected
- **COMPONENT**: Specific component failures or issues

### Alert Management

```python
from src.validation.qa_dashboard import QAAlertSystem

alert_system = QAAlertSystem(validation_database)

# Check for alerts after validation
alert_system.check_and_generate_alerts(validation_results, run_id)

# Get alert summary
alert_summary = alert_system.get_alert_summary()
```

## üîÑ Integration with Existing Pipeline

### Pre-processing Integration

```python
# Add validation to DEM processing
dem_processor = DEMProcessor()
dem_validator = DEMValidator()

processed_dem = dem_processor.process(raw_dem_path)
validation_result = dem_validator.validate(processed_dem)

if validation_result.status == 'FAIL':
    raise ValueError(f"DEM validation failed: {validation_result.issues}")
```

### ML Training Integration

```python
# Add validation to training pipeline
def train_model_with_validation(dataset, model, config):
    # Validate dataset before training
    data_validator = MLDataValidator(config['validation']['data'])
    data_result = data_validator.validate(dataset)
    
    if data_result.status == 'FAIL':
        raise ValueError("Dataset validation failed")
    
    # Train model
    training_results = train_model(dataset, model, config)
    
    # Validate training results
    perf_validator = ModelPerformanceValidator(config['validation']['performance'])
    perf_result = perf_validator.validate(training_results)
    
    return training_results, [data_result, perf_result]
```

## üìã Best Practices

### Validation Workflow

1. **Early Validation**: Validate data as early as possible in the pipeline
2. **Component Isolation**: Validate individual components before integration
3. **Incremental Validation**: Add validation checks incrementally
4. **Automated Integration**: Integrate validation into CI/CD pipelines
5. **Regular Monitoring**: Run validation checks regularly, not just on new data

### Performance Optimization

1. **Sampling Strategy**: Use representative sampling for large datasets
2. **Parallel Processing**: Run independent validations concurrently
3. **Caching**: Cache validation results for repeated analyses
4. **Progressive Validation**: Stop early on critical failures
5. **Resource Monitoring**: Monitor memory and CPU usage during validation

### Quality Thresholds

1. **Conservative Thresholds**: Start with conservative validation thresholds
2. **Data-Driven Adjustment**: Adjust thresholds based on historical data
3. **Context-Specific**: Use different thresholds for different use cases
4. **Regular Review**: Review and update thresholds regularly
5. **Domain Expertise**: Incorporate domain knowledge in threshold setting

## üîÆ Future Enhancements

### Planned Features

- **Real-time Streaming Validation**: Validate data streams in real-time
- **Advanced ML Validation**: More sophisticated model validation techniques
- **Cloud Integration**: Deploy validation framework to cloud environments
- **API Endpoints**: RESTful API for validation services
- **Advanced Visualizations**: More interactive and detailed visualizations

### Integration Opportunities

- **MLflow Integration**: Track validation results alongside ML experiments
- **Kubeflow Pipelines**: Integrate validation steps into ML pipelines
- **Apache Airflow**: Schedule and orchestrate validation workflows
- **Prometheus/Grafana**: Export metrics for monitoring dashboards

## ü§ù Contributing

### Development Setup

```bash
# Clone repository
git clone <repository-url>
cd FloodRisk

# Install dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt

# Install package in development mode
pip install -e .

# Run tests
pytest tests/validation/ -v
```

### Adding New Validators

1. Create validator class inheriting from `BaseValidator`
2. Implement the `validate()` method
3. Add comprehensive tests
4. Update documentation
5. Add integration examples

### Code Style

- Follow PEP 8 style guidelines
- Use type hints for all function parameters and return values
- Include comprehensive docstrings
- Add logging for debugging and monitoring
- Write comprehensive tests for new features

## üìö References

- [FloodRisk ML Pipeline Documentation](../README.md)
- [Validation Metrics Reference](validation_metrics.md)
- [API Documentation](api_documentation.md)
- [Testing Guidelines](testing_guidelines.md)

## üìû Support

For questions, issues, or contributions:

- **GitHub Issues**: Report bugs and feature requests
- **Documentation**: Check existing documentation first
- **Email**: Contact the FloodRisk development team
- **Wiki**: Additional resources and examples

---

**FloodRisk Validation Framework** - Ensuring data quality and ML pipeline reliability for flood risk modeling.